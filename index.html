<!DOCTYPE html>
<html>
<head>
	<title>Chuck Bradley's website</title>
	<link rel="shortcut icon" href="images/chs-1.png" type="image/png">
	<link rel="stylesheet" href="css/style.css">
</head>
<body>
<div class="header">
<h1>Chuck Bradley</h1>
bradley4 at purdue dot edu

</div>

<div class="main" id="About">
<h2>About</h2>
<p>I recently earned my PhD in Linguistics from Purdue University (May 2019), where I studied with Ronnie Wilbur in the Sign Language and Linguistics Lab.</p>
<p> I am interested in the emergence of grammatical features in L/language via iconicity, or, a motivated connection between how a sign (or word, gesture, etc.) looks/ sounds and what it means.
My current work explores perceptual features that guide argument structure de/coding in pantomime and ASL signs. By applying (modified) linguistic tests to pantomime, my goal is to characterize how structure arises in developing (sign) languages, and ultimately become codified. My approach considers the contributions of both syntactic (transitivity, alternations/ constructions) and semantic (e.g., tool use, manner verbs) notions of argument structure. Distributional or cognitive explanations are als explored (e.g., transitivity prominence, agent prominence). In future work, I will explore the neural correlates of gestural and linguistic argument structure in production and perception. </p>

<p>My previous work focused on linguistic constraints on 'gestural' elements in ASL verbs of motion and location, making a critical comparison between information available in classifier constructions (aka, depicting verbs, etc.) in American Sign Language and in 'regular' motion verbs. While classifier constructions allow for much richer spatial descriptions than regular motion verbs, they are still subject to cross-linguistic constraints on (spatial) information packaging.</p>

<p>Outside of this, I pick at an outstanding puzzle relating the syntax and semantics of adnominal modifiers, specifically adjectives in APs and adjectives/ nouns in synthetic compounds. The similar constellation of semantic interpretations  argues for a similar syntax between APs and synthetic compounds, contrary to fact.  </p>

<p>In my free time, I like to cycle and garden. In rainier months, I burn through classic and world cinema.</p>
</div>

<div class="main" id="Publications">
<h2>Publications</h2>
<p>
<em>Submitted</em><br>
Bradley, C., Malaia, E., Siskind, J. M. and R. B. Wilbur. Visual form of ASL verb signs predicts non-signer judgment of transitivity. [<a href="https://docs.google.com/document/d/1E9aCumsuqHxJnbS_FMq1ojxrpunUzoFxHnQuDDUVabg/edit?usp=sharing" target="_blank">MS</a>]
</p>
<p>
<em>In progress</em><br>
Bradley, C., Malaia, E. and R. B. Wilbur. Iconicity of transitivity distinctions in ASL classifier constructions and the pantomimes of hearing non-signers.
</p>
<p>Bradley, C. Semantic contributions to handshape manipulation in the expression of argument structure in pantomime. </p>

<p>
<em>Presentations</em><br>
Bradley, C. (2018) Can formal features be predicted from form? Using Machine Learning to predict transitivity class from the form of pantomime and ASL classifier constructions. 
Poster presented at Formal and Experimental Advances in Sign Language Theory 7. Venice, IT. [<a href="pdfs/FEAST18_transitivePhonology.pdf" target="_blank">abstract</a>]
</p>
<p>
Bradley, C., Siskind, J.M., and R. Wilbur. (2017) Neural representation of minimal syntactic units. 
Poster presented at Cognitive Computational Neuroscience 1. New York, NY, USA. [<a href="pdfs/ccn2017_merge.pdf" target="_blank">abstract</a>]
</p>
Bradley, C. and H. Nassar. (2017) Rapid processing of ELAN data: quick and dirty numbers for statistical analysis. 
Poster presented at Formal and Experimental Approaches to Sign Language Theory 6. Reykjavik, Iceland.  [<a href="pdfs/FEAST17_computation.pdf" target="_blank">abstract</a>]
</p>
<p>
Bradley, C. and V. Lee-Shoenfeld. (2010) A theoretical look at the Person Agreement Marker in German Sign Language. 
Poster presented at Theoretical Issues in Sign Language Research 10. West Lafayette, IN, USA.
</p>

<p>
<em>Theses</em><br>
Bradley, C. (2019). Transparency of transitivity in pantomime, sign language [<a href="pdfs/bradley-diss-final.pdf" target="_blank">Full</a>]
</p>
<p>
Bradley, C. (2013). Motion events and event segmentation in American Sign Language [<a href="pdfs/bradley-ma-final.pdf" target="_blank">Full</a>]
</p>
<p>
Bradley, C. (2009). A theoretical look at the Person Agreement Marker in German Sign Language.
</div>

<div class="main" id="Projects">
<h2>Projects</h2>

<p>See my <a href="https://github.com/" target="_blank">GitHub &#8599;</a> for source code.  Click project names to expand/ collapse description. All are scripted in Python.</p>

<p>

<a href="#/" onclick="toggle_visibility('trans');"><em>Translucency/ Transparency</em></a><tag >#sign language</tag> <tag>#experiment</tag> <tag>#NLP</tag> <tag>#basic inferential statistics</tag><br> 
	<div class = "row" id="trans" style="display:none;">
		<div class = "column">
	This project investigates whether transparency scores correlate with translucency scores. Many signs in American Sign Language (ASL) are said to be iconic with respect to their lexical meaning. The sign CAT, for instance, is signed by tracing the outline of whiskers. Non-signers will agree that the sign CAT looks like its meaning when presented with its equivalent English word, but are less likely to accurately name the sign without the translation (Klima and Bellugi, 1979). A sign is said to be translucent if non-signers agree that it looks like what it means with the translation. A sign is said to be transparent if non-signers can accurately name the sign without translation.
	
	We replicate this finding by asking non-signers to label verbs that vary with respect to their translucency (these measures were obtained from ASL-LEX.org), selecting 15 verbs that have low, medium, and high iconicity scores (5 at each level). However, instead of scoring '1' HIT and '0' MISS, we use a measure of the semantic similarity (obtained from wordnet). Preliminary results are dichotomous, with items with low and medium translucency scores having very low transparency scores, and items with high translucency scores having high transparency scores.
		</div>
		<div class = "columna">
		<img src="images/cat.png" style="width:30%;height:auto"> <img src="images/yes-cat.png" style="width:30%;height:auto"> <img src="images/yes-cat-2.png" style="width:30%;height:auto"> <p>
		<img src="images/transparency.png" style="max-width: 100%; height: auto;"/>
		</div>
	</div>
	
</p><p>
	
	<a href="#/" onclick="toggle_visibility('elan');"><em>ELAN Overlap</em></a> <tag>#sign language</tag> <tag> #annotation</tag> <tag>#html/xml processing</tag> <br> 
	<div class = "row" id="elan" style="display:none;">
		<div class = "column">
	A little program designed to extract simultaneous annotations in the video annotation software, ELAN [ <a href="https://tla.mpi.nl/tools/tla-tools/elan/" target="_blank">link &#8599;</a> ]. The bottom left image shows a snippet of the ELAN interface, demonstrating simultaneous annotations. 
	Given an .eaf file and two tiers of interest, the program reports pairwise overlaps, including (a) the two overlapping annotations, (b) the overlap type (containment, overlap of one edge or the other, etc.), and (c) the duration of the overlap.
	Results are returned in a table (dataframe) for easy exploration (bottom right image).
	<p>
	<img src="images/elan.PNG" style="max-width: 30%; height: auto;"/>  <img src="images/elan2.PNG" style="max-width: 40%; height: auto;"/>
	</p>
		</div>
		<div class = "columna">
		</div>
	</div>
	
</p><p>
	
	<a href="#/" onclick="toggle_visibility('icon');"><em>ASL-LEX iconicity</em></a> <tag>#sign language</tag> <tag>#corpus</tag> <tag>#machine learning</tag> <tag>#(logistic) regression</tag> <br> 
	<div class = "row" id="icon" style="display:none;">
		<div class = "column">
	Is lexical category information apparent in the form of signs from American Sign Language (ASL)? This project correlates visual (iconic) features of ASL signs with lexical/ grammatical category information using corpus information downloaded from ASL-LEX.org [<a href="https://www.asl-lex.org" target="_blank"> link &#8599;</a> ]. Such features include whether the sign is two handed, the sign's handshape, and the movement of the sign. Features were percolated into an 8-fold leave-one-out cross-validation paradigm using a logistic regression classifier. Results indicate that visual features only weakly predict sign category membership.
	
	<p> The project also considers lexical information, e.g., <i>Do verb signs exhibit greater handshape (phonological) neighborhood density than noun signs?</i> Classifier performance was worse trained on lexical features than on visual features. </p>
	
	<p>This project contributes to our understanding of how visual or lexical information does or does not bootstrap language learning (here, learning the grammatical class of a sign) in the visual/ manual modality. However, the analysis suffers from a small, unbalanced dataset, whereas similar studies on spoken languages have exponentially more datapoints.</p>

		</div>
		<div class = "columna">

		</div>
	</div>
	
</p><p>
	
	<a href="#/" onclick="toggle_visibility('tracker');"><em>simple-tracker</em></a> <tag>#opencv (computer vision)</tag> <br> 
		<div class = "row" id="tracker" style="display:none;">
		<div class = "column">
	
		<p>Computes Lucas Kanade optical flow for a user-defined point within a user drawn window. The algorithm attempts to track that point across all frames in the video. The points total displacement is overlaid on top of the video, as well as the path it traces. An arbitrary number of points may be tracked. </p>
		
		<p>Keyboard controls allow users to progress frames in the video until the desired point is in view. For instance, if tracking hands, users may progress the video until the subject's hands are in view (pictured to the right).</p>
		
		<p>This tool is potentially useful for quantifying hand displacement or relative hand movement in videos recorded without motion capture. Gross displacement, for example, may be related to aspectual information (e.g. telic vs. atelic or  perfective vs. imperfective predicates) and relative movement may be related to plural concepts. </p>
	
		</div>
		<div class = "columna">
		<img src="images/out.gif" style="max-width: 100%; height: auto;"/>
		</div>
	</div>

</p><p>
	
<a href="#/" onclick="toggle_visibility('segment');"><em>video-segment</em></a> <tag>#opencv (computer vision)</tag> <br>
		<div class = "row" id="segment" style="display:none;">
		<div class = "column">
		This short program calculates pixel-wise differences in intensity to detect subject movement. Signs, gestures, self-adapters, etc. are identified using a simple heuristic (i.e., differences that are over two standard deviations from the mean are likely relevant). Three snippets are included below. The first is a tracing gesture, the second is the subject progressing to the next stimulus item, and the third is a manner gesture. Parameters (e.g., peak difference, peak window size, etc.) can be tweaked to try to avoid clipping irrelevant movements. 
		<p>
		<img src="images/np_0.gif" style="max-width: 30%; height: auto;"/> <img src="images/np_1.gif" style="max-width: 30%; height: auto;"/> <img src="images/np_3.gif" style="max-width: 30%; height: auto;"/>
		</p>
		</div>
		<div class = "columna">
		
		</div>
		</div>
</p>

</div>

<div class="main" id="CV">
<h2>CV</h2>
<p>A .pdf of my CV can be found here [<a href="pdfs/cv.pdf" target="_blank">CV</a>] (Updated: 30 Oct, 2019)</p>
<p>
<em>PhD in Linguistics, Purdue University, 2019</em>
<br>Major professor: Ronnie Wilbur
<br>Committee: Jeffrey Mark Siskind, Evie Malaia, Elaine Francis
</p>
<p>
<em>MA in Linguistics, Purdue University, 2013</em>
<br>Major professor: Ronnie Wilbur
<br>Committee: Elena Benedicto, Elaine Francis
</p>
<p>
<em>BA in Linguistics, Middlebury College, 2009</em>
<br>Major professors: Christopher Star, Vera Lee-Schoenfeld
<br>Thesis: A theoretical look at the Person Agreement Marker in German Sign Language
<br>Major: Linguistics, cum laude
<br>Minor: Latin

</p>
</div>


<div id="mySidenav" class="sidenav">
	<div class="image-cropper">
	<img src="images/me.png" class="rounded" />
	</div>
	<a href="#About">About</a>
  <a href="#Publications">Publications</a>
  <a href="#Projects">Projects</a>
  <a href="#CV">CV</a>
  <!--<a href="#">Teaching</a>-->
  <a href="https://github.com/C-huck/" target="_blank">GitHub &#8599;</a>
</div>



<script type="text/javascript">
<!--
    function toggle_visibility(id) {
       var e = document.getElementById(id);
       if(e.style.display == 'block')
          e.style.display = 'none';
       else
          e.style.display = 'block';
    }
//-->
</script>

</body>
</html>




   

