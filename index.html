<!DOCTYPE html>
<html>
<head>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-155795636-1"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-155795636-1');
	</script>
	<title>Chuck Bradley</title>
	<link rel="shortcut icon" href="images/chs-1.png" type="image/png">
	<link rel="stylesheet" href="css/style.css">
</head>
<body>
<div class="header">
<h1>Chuck Bradley, PhD</h1>
bradley4 at purdue dot edu

</div>

<div class="main" id="About">
<h2>About</h2>
<p>I earned my PhD in Linguistics from Purdue University, where I studied with Ronnie Wilbur in the Sign Language and Linguistics Lab.</p>
<p> I am interested in the emergence of grammatical features in L/language via iconicity, or, a motivated connection between how a word (or sign or gesture) looks like and what it means.
My current work explores perceptual features that guide argument structure encoding and decoding in silent gesture (pantomime) and in signs from natural sign languages.
By applying modified linguistic tests to pantomime, my goal is to characterize how structure arises in developing sign languages, and ultimately become codified.
<!--My approach considers the contributions of both syntactic and semantic notions of argument structure.
Distributional and cognitive explanations are also explored (e.g., transitivity prominence, agent prominence).
In future work, I will explore the neural correlates of gestural and linguistic argument structure in production and perception. --></p>

<p>My previous work focused on linguistic constraints on 'gestural' elements in ASL verbs of motion and location, making critical comparisons
between information available in classifier constructions (aka, depicting verbs, etc.) in American Sign Language and in lexical motion verbs.
While classifier constructions allow for much richer spatial descriptions than regular motion verbs,
they are still subject to cross-linguistic constraints on (spatial) information packaging.</p>

<p>Outside of this, I pick at an outstanding puzzle relating the syntax and semantics of adnominal modifiers, specifically adjectives in APs and adjectives/ nouns in synthetic compounds. The similar constellation of semantic interpretations  argues for a similar syntax between APs and synthetic compounds, contrary to fact.  </p>

<p>In my free time, I like to cycle and garden. In rainier months, I burn through classic and world cinema.</p>

	</div>
<div class="main" id="Education">
<h2>Education</h2>
	</p><p>
<em>PhD in Linguistics, Purdue University, 2019</em>
<br>Major professor: Ronnie Wilbur
<br>Committee: Jeffrey Mark Siskind, Evie Malaia, Elaine Francis
<br>Thesis: Transparency of transitivity in pantomime, sign language
</p>
<p>
<em>MA in Linguistics, Purdue University, 2013</em>
<br>Major professor: Ronnie Wilbur
<br>Committee: Elena Benedicto, Elaine Francis
<br>Thesis: Motion events and event segmentation in American Sign Language
</p>
<p>
<em>BA in Linguistics, Middlebury College, 2009</em>
<br>Major professors: Christopher Star, Vera Lee-Schoenfeld
<br>Thesis: A theoretical look at the Person Agreement Marker in German Sign Language
<br>Major: Linguistics, cum laude
<br>Minor: Latin
</p>

</div>

<div class="main" id="Publications">
<h2>Publications</h2>
<p>
	<em>Submitted</em><br>
	Bradley, C., Malaia, E., Siskind, J. M. and R. B. Wilbur. Visual form of ASL verb signs predicts non-signer judgment of transitivity.
	[<a href="https://docs.google.com/document/d/1ysZn8jcSVibMp0_VchefXnhC6TxCe2ZIIERrPACvoHQ/edit?usp=sharing" target="_blank">MS &#8599;</a>]
</p>
<p>
	Karabuklu, S., Wood, S. Sandra , Bradley, C., Wilbur, R. B. , and E. A. Malaia. Sign language learning increases temporal resolution of visual attention.
	[<a href="https://drive.google.com/file/d/1lW2gA0AMkKBXfwuwygOj13Ajia-YAXLb/view?usp=sharing" target="_blank">MS &#8599;</a>]

</p>
<p>
	<em>In progress</em><br>
	Bradley, C. Visual form and event semantics predict transitivity in pantomimed actions: Evidence for compositionality.
	[<a href="https://docs.google.com/document/d/1OiYknCBRLQxD7zS68Q_yIqU_QCEYDRbUg4jz_H-aHYg/edit?usp=sharing" target="_blank">Early draft &#8599;</a>] </p>

	</p>
<!--<p>
Bradley, C., Malaia, E. and R. B. Wilbur. Iconicity of transitivity distinctions in ASL classifier constructions and the pantomimes of hearing non-signers.
</p>-->


<p>
<em>Presentations</em><br>
Bradley, C. (2021). Compositionality in holistic pantomime characterizes a gesture-first protolanguage. Talk to be given at <i>Expression, Language, and Music</i> 1. Hartford, CT,
USA.
</p>

<p>
	 Evidence for argument structure in the form of pantomime. Poster presented at <i>Experiments in Linguistic Meaning</i> 1. Philadelphia, PA, USA. September, 2020.
</p>
<p>
Bradley, C. (2020). Evidence for subunit structure when gesturers communicate in/transitive actions.
Poster presented at the <i>CUNY Conference on Human Sentence Processing</i> 33. Amherst, MA, USA.
[<a href="pdfs/CUNY2020_bradley.pdf" target="_blank">abstract &#8599;</a>]

</p>
Bradley, C. (2018) Can formal features be predicted from form? Using Machine Learning to predict transitivity class from the form of pantomime and ASL classifier constructions.
Poster presented at <i>Formal and Experimental Advances in Sign Language Theory</i> 7. Venice, IT. [<a href="pdfs/FEAST18_transitivePhonology.pdf" target="_blank">abstract &#8599;</a>]
</p>
<p>
Bradley, C., Siskind, J.M., and R. Wilbur. (2017) Neural representation of minimal syntactic units.
Poster presented at <i>Cognitive Computational Neuroscience</i> 1. New York, NY, USA. [<a href="pdfs/ccn2017_merge.pdf" target="_blank">abstract &#8599;</a>]
</p>
Bradley, C. and H. Nassar. (2017) Rapid processing of ELAN data: quick and dirty numbers for statistical analysis.
Poster presented at <i>Formal and Experimental Approaches to Sign Language Theory</i> 6. Reykjavik, Iceland.  [<a href="pdfs/FEAST17_computation.pdf" target="_blank">abstract &#8599;</a>]
</p>
<p>
Bradley, C. and V. Lee-Shoenfeld. (2010) A theoretical look at the Person Agreement Marker in German Sign Language.
Poster presented at <i>Theoretical Issues in Sign Language Research</i> 10. West Lafayette, IN, USA.
</p>

<p>
<em>Theses</em><br>
Bradley, C. (2019). Transparency of transitivity in pantomime, sign language [<a href="pdfs/bradley-diss-final.pdf" target="_blank">Full &#8599;</a>] [<a href="#/" onclick="toggle_visibility('diss');"> Summary</a>] <br>
		<div class = "row" id="diss" style="display:none;">
		 The goal of the project was to uncover similarities
in form-meaning correspondence between sign language and pantomime with respect to transitivity coding. In brief, I elicited pantomimes and classifier constructions (a subset of highly
imagistic signs in American Sign Language [ASL]) from non-signers and a signer, respectively,
that show the manipulation or movement of everyday objects. I annotated these productions
for phonetic (purely visual) and phonological (visual, but organized by the grammar of ASL)
features and correlated them with transitivity labels (i.e., manipulation: transitive; movement:
intransitive). I then used Amazon Mechanical Turk (AMT) to obtain transitivity judgments
from non-signers, and correlated these judgments with features of the stimuli as well.
Several features accurately predicted both actual and perceived transitivity, suggesting that: (a)
without explicit training, non-signers recruit the same visual features for encoding and decoding
transitivity in pantomime; and (b) the encoding of transitivity in classifier constructions and
pantomime can be predicted using these same features. This suggests that these features have
their roots in more domain general cognitive processes (e.g., vision, manual praxis), and suggests
specific parameters for communicative strategies that collocutors try or principally do not try.
The full dissertation can be accessed via the link above.
		</div>
</p>
<p>
Bradley, C. (2013). Motion events and event segmentation in American Sign Language [<a href="pdfs/bradley-ma-final.pdf" target="_blank">Full &#8599;</a>] [<a href="#/" onclick="toggle_visibility('ma');"> Summary</a>] <br>
		<div class = "row" id="ma" style="display:none;">
		This project examined verbs of motion and location in American Sign Language (ASL) in light of semantic limits on the amount of spatial information spoken languages exhibit.
		Such verbs in ASL come in two sorts, lexical verbs (akin to words in spoken languages) and classifier constructions (aka depicting verbs), a set of highly iconic, polymorphemic signs.
		It was found that lexical verbs pattern like verbs of motion/ location in spoken languages, while classifier constructions could provide much richer spatial descriptions.
		The project provides some discussion on whether path and location are gestural, linguistic or both in classifier constructions, but ultimately concludes that Language is not fully described without the inclusion of possibilities that are uniquely available to sign languages.

		</div>
</p>
<p>
Bradley, C. (2009). A theoretical look at the Person Agreement Marker in German Sign Language. [<a href="#/" onclick="toggle_visibility('thesis');"> Summary</a>] <br>
		<div class = "row" id="thesis" style="display:none;">
		This paper provided a syntactic treatment of the agreement marker, PAM, in German Sign Language.
		<!--Specifically, this paper idetified a problem with the strictly modular, and strict unidirectional flow of the Grammar -->
		Specifically, despite Spell-Out traditionally being the locus of agreement phenomena (e.g., in Minimalism and Distributed Morphology), PAM interacts syntactically.
		I explained PAM's different linear positions by appealing to Remnant Movement, though the syntactic behavior of a post-syntactic agreement element was ultimately left unexplained.
		</div>
</div>

<div class="main" id="Projects" style="width:90%">
<h2>Projects</h2>

<p>Click project names to expand/ collapse description. See my <a href="https://github.com/C-huck/" target="_blank">GitHub &#8599;</a> for source code. All are scripted in Python.
	I contributed all conceptualization, planning and coding. </p>

<p>

<a href="#/" onclick="toggle_visibility('trans');"><em>Translucency/ Transparency</em></a><tag >#sign language</tag> <tag>#experiment</tag> <tag>#NLP</tag> <tag>#basic inferential statistics</tag><br>
	<div class = "row" id="trans" style="display:none;">
		<div class = "column">
	This project investigates whether transparency scores correlate with translucency scores. Many signs in American Sign Language (ASL) are said to be iconic with respect to their lexical meaning. The sign CAT, for instance, is signed by tracing the outline of whiskers. Non-signers will agree that the sign CAT looks like its meaning when presented with its equivalent English word, but are less likely to accurately name the sign without the translation (Klima and Bellugi, 1979). A sign is said to be translucent if non-signers agree that it looks like what it means with the translation. A sign is said to be transparent if non-signers can accurately name the sign without translation.

	We replicate this finding by asking non-signers to label verbs that vary with respect to their translucency (these measures were obtained from ASL-LEX.org), selecting 15 verbs that have low, medium, and high iconicity scores (5 at each level). However, instead of scoring '1' HIT and '0' MISS, we use a measure of the semantic similarity (obtained from wordnet). Preliminary results are dichotomous, with items with low and medium translucency scores having very low transparency scores, and items with high translucency scores having high transparency scores.
		</div>
		<div class = "columna">
		<img src="images/cat.png" style="width:30%;height:auto"> <img src="images/yes-cat.png" style="width:30%;height:auto"> <img src="images/yes-cat-2.png" style="width:30%;height:auto"> <p>
		<img src="images/transparency.png" style="max-width: 100%; height: auto;"/>
		</div>
	</div>

</p><p>

	<a href="#/" onclick="toggle_visibility('elan');"><em>ELAN Overlap</em></a> <tag>#sign language</tag> <tag> #annotation</tag> <tag>#html/xml processing</tag> <br>
	<div class = "row" id="elan" style="display:none;">
		<div class = "column">
	A little program designed to extract simultaneous annotations in the video annotation software, ELAN [ <a href="https://tla.mpi.nl/tools/tla-tools/elan/" target="_blank">link &#8599;</a> ]. The bottom left image shows a snippet of the ELAN interface, demonstrating simultaneous annotations.
	Given an .eaf file and two tiers of interest, the program reports pairwise overlaps, including (a) the two overlapping annotations, (b) the overlap type (containment, overlap of one edge or the other, etc.), and (c) the duration of the overlap.
	Results are returned in a table (dataframe) for easy exploration (bottom right image).
	<p>
	<img src="images/elan.PNG" style="max-width: 30%; height: auto;"/>  <img src="images/elan2.PNG" style="max-width: 40%; height: auto;"/>
	</p>
		</div>
		<div class = "columna">
		</div>
	</div>

</p><p>

	<a href="#/" onclick="toggle_visibility('icon');"><em>ASL-LEX iconicity</em></a> <tag>#sign language</tag> <tag>#corpus</tag> <tag>#machine learning</tag> <tag>#(logistic) regression</tag> <br>
	<div class = "row" id="icon" style="display:none;">
		<div class = "column">
	Is lexical category information apparent in the form of signs from American Sign Language (ASL)? This project correlates visual (iconic) features of ASL signs with lexical/ grammatical category information using corpus information downloaded from ASL-LEX.org [<a href="https://www.asl-lex.org" target="_blank"> link &#8599;</a> ]. Such features include whether the sign is two handed, the sign's handshape, and the movement of the sign. Features were percolated into an 8-fold leave-one-out cross-validation paradigm using a logistic regression classifier. Results indicate that visual features only weakly predict sign category membership.

	<p> The project also considers lexical information, e.g., <i>Do verb signs exhibit greater handshape (phonological) neighborhood density than noun signs?</i> Classifier performance was worse trained on lexical features than on visual features. </p>

	<p>This project contributes to our understanding of how visual or lexical information does or does not bootstrap language learning (here, learning the grammatical class of a sign) in the visual/ manual modality. However, the analysis suffers from a small, unbalanced dataset, whereas similar studies on spoken languages have exponentially more datapoints.</p>

		</div>
		<div class = "columna">

		</div>
	</div>

</p><p>

	<a href="#/" onclick="toggle_visibility('tracker');"><em>simple-tracker</em></a> <tag>#opencv #optical flow #computer vision</tag> <br>
		<div class = "row" id="tracker" style="display:none;">
		<div class = "column">

		<p>Computes Lucas Kanade optical flow for a user-defined point within a user drawn window. The algorithm attempts to track that point across all frames in the video. The points total displacement is overlaid on top of the video, as well as the path it traces. An arbitrary number of points may be tracked. </p>

		<p>Keyboard controls allow users to progress frames in the video until the desired point is in view. For instance, if tracking hands, users may progress the video until the subject's hands are in view (pictured to the right).</p>

		<p>This tool is potentially useful for quantifying hand displacement or relative hand movement in videos recorded without motion capture. Gross displacement, for example, may be related to aspectual information (e.g. telic vs. atelic or  perfective vs. imperfective predicates) and relative movement may be related to plural concepts. </p>

		</div>
		<div class = "columna">
		<img src="images/out.gif" style="max-width: 100%; height: auto;"/>
		</div>
	</div>

</p><p>

<a href="#/" onclick="toggle_visibility('segment');"><em>video-segment</em></a> <tag>#opencv</tag> <br>
		<div class = "row" id="segment" style="display:none;">
		<div class = "column">
		This short program calculates pixel-wise differences in intensity to detect subject movement. Signs, gestures, self-adapters, etc. are identified using a simple heuristic (i.e., differences that are over two standard deviations from the mean are likely relevant). Three snippets are included below. The first is a tracing gesture, the second is the subject progressing to the next stimulus item, and the third is a manner gesture. Parameters (e.g., peak difference, peak window size, etc.) can be tweaked to try to avoid clipping irrelevant movements.
		<p>
		<img src="images/np_0.gif" style="max-width: 30%; height: auto;"/> <img src="images/np_1.gif" style="max-width: 30%; height: auto;"/> <img src="images/np_3.gif" style="max-width: 30%; height: auto;"/>
		</p>
		</div>
		<div class = "columna">

		</div>
		</div>
</p>

</div>

<div class="main" id="Teaching" style="width:90%">
<h2>Teaching</h2>

	<em> Purdue University</em>
	<p>
	as <b> primary instructor</b>:<br>
	(All materials created by me)<br>
	<ul style="list-style-type: none; padding: 10">
	<li>Syntax and Semantics (3 semesters)<br>
	[<a href="pdfs/syllabus-ling321_18.pdf" target="_blank">sample syllabus &#8599;</a>]
	[<a href="https://docs.google.com/presentation/d/1kwS0EYiIHFKjoyyKifi6d2SfvA7IyFtTMjB4TS28T3I/edit?usp=sharing" target="_blank">sample slide &#8599;</a>]
	[<a href="pdfs/ling321s2018-final.pdf" target="_blank">sample exam &#8599;</a>]
	</li><li>
	Introduction to Linguistics (2 semesters)<br>
	[<a href="pdfs/syllabus-slhs227_15.pdf" target="_blank">sample syllabus &#8599;</a>]
	[<a href="https://docs.google.com/presentation/d/1TcW3-4DGnVzAWO8976dEH5rcbSzNRipyqbswm-2Y69o/edit?usp=sharing" target="_blank">sample slide &#8599;</a>]
	[<a href="#" target="_blank">sample assignment &#8599;</a>]
	[<a href="https://docs.google.com/document/d/10z2eXm8G2Zp3E3McA3MqkZE_yW3UShXu34RxEYWJREM/edit?usp=sharing" target="_blank">sample exam &#8599;</a>]
		</li></ul>
		</p><p>
	as <b> teaching assistant</b>:<br>
	<ul style="list-style-type: none; padding: 10">
	<li>American Deaf Community: Language, Culture, and Society, Purdue University<br>
	American Sign Language (Levels 1, 2, 4)<br>
	Introduction to Linguistics
		</li></ul></p>
</div>

<div class="main" id="CV">
<h2>CV</h2>
<p>A .pdf of my CV can be found here [<a href="pdfs/cv-bradley-091820.pdf" target="_blank">CV</a>] (Updated: 18 Sept, 2020)</p>


</div>


<div id="mySidenav" class="sidenav">
	<div class="image-cropper">
	<img src="images/me.png" />
	</div>
	<a href="#About">About</a>
	<a href="#Education">Education</a>
  <a href="#Publications">Publications</a>
  <a href="#Projects">Projects</a>
  <a href="#Teaching">Teaching</a>
	<a href="#CV">CV</a>
  <a href="https://github.com/C-huck/" target="_blank">GitHub &#8599;</a>
</div>



<script type="text/javascript">
<!--
    function toggle_visibility(id) {
       var e = document.getElementById(id);
       if(e.style.display == 'block')
          e.style.display = 'none';
       else
          e.style.display = 'block';
    }
//-->
</script>

</body>
</html>
